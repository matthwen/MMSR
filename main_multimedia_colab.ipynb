{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "#!pip install umap\n",
    "#from umap import UMAP\n",
    "import pandas as pd\n",
    "import os \n",
    "import csv\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn import neighbors\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from time import time\n",
    "from sklearn.manifold import TSNE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    ID           artist                   song  \\\n",
      "0     ZEexJX2SztZYrOkD     Roger Waters                Déjà Vu   \n",
      "1     nioxFqknx3fom1hP      Iron Maiden               Invaders   \n",
      "2     B9jCiktNXTYQXrE7    The Offspring  Jennifer Lost the War   \n",
      "3     XXz68KfaEjOJF7bw           Health           KNIGHTRIDERS   \n",
      "4     QM8qykYFnIFYfJvS  Freeze Corleone             Jeremy Lin   \n",
      "...                ...              ...                    ...   \n",
      "3673  G6IjeJK8Hyyyz07u     Light Asylum          Shallow Tears   \n",
      "3674  ZqiBxHOfpB9baZRA     mewithoutYou      East Enders Wives   \n",
      "3675  V8hJK8mWXccbpxyX     Nina Nesbitt          Last December   \n",
      "3676  m7gADmFpdvANqa6k             Ride           Kaleidoscope   \n",
      "3677  jBInKb9WlU5RgwMJ              Sia     Midnight Decisions   \n",
      "\n",
      "                                                  album  popularity  release  \\\n",
      "0                      Is This The Life We Really Want?        46.0     2017   \n",
      "1             The Number of the Beast (2015 - Remaster)        35.0     1982   \n",
      "2                                         The Offspring        33.0     1989   \n",
      "3     Grand Theft Auto Online: Arena War (Official S...        19.0     2019   \n",
      "4                                      Projet Blue Beam        46.0     2018   \n",
      "...                                                 ...         ...      ...   \n",
      "3673                                       Light Asylum        31.0     2012   \n",
      "3674                                        Ten Stories        18.0     2012   \n",
      "3675      The Sun Will Come up, The Seasons Will Change        39.0     2019   \n",
      "3676                                 Nowhere (Expanded)        27.0     1990   \n",
      "3677                    This Is Acting (Deluxe Version)        46.0     2016   \n",
      "\n",
      "      danceability  energy  key  mode  valence    tempo  duration_ms  \n",
      "0            0.551   0.328  7.0   1.0   0.1800  129.130       267453  \n",
      "1            0.439   0.937  4.0   0.0   0.6840  111.794       203387  \n",
      "2            0.350   0.774  9.0   1.0   0.4790  114.672       155360  \n",
      "3            0.533   0.885  1.0   1.0   0.0333  118.045       168400  \n",
      "4            0.782   0.431  9.0   0.0   0.1210  134.040       230000  \n",
      "...            ...     ...  ...   ...      ...      ...          ...  \n",
      "3673         0.519   0.889  2.0   1.0   0.0660  118.000       308840  \n",
      "3674         0.562   0.610  2.0   0.0   0.2280  133.581       172187  \n",
      "3675         0.615   0.254  4.0   1.0   0.2930  122.051       295587  \n",
      "3676         0.148   0.881  9.0   0.0   0.5100  161.731       181107  \n",
      "3677         0.591   0.658  7.0   0.0   0.0553  112.360       222973  \n",
      "\n",
      "[3678 rows x 13 columns]\n",
      "                   ID                  artist                   song  \\\n",
      "0    LvRgn4QRFOlJhiKN               Puma Blue              Soft Porn   \n",
      "1    ibpcxyHvTMnfx5M6          Injury Reserve          Whatever Dude   \n",
      "2    cpYAjiUPOXHAsN2O           Enter Shikari           Live Outside   \n",
      "3    uQOxRTDAwSPqg0iE                  T. Rex                 Debora   \n",
      "4    fRhMRe2SpHMrgJ8h               Hüsker Dü       Eight Miles High   \n",
      "..                ...                     ...                    ...   \n",
      "915  Q3fJPbY4Ghbv1YYC            Jeff Buckley          Witches' Rave   \n",
      "916  yIul7Xf9NqZByUX7         Collective Soul  Where the River Flows   \n",
      "917  vqANcZIrK0vNveYR  Life Without Buildings               New Town   \n",
      "918  aYbeIeLYrFWl2pwr                 MV Bill       Soldado do Morro   \n",
      "919  5ArAw1CKRcINcQyU           Gusttavo Lima       Balada - Ao Vivo   \n",
      "\n",
      "                                                 album  release  danceability  \\\n",
      "0                                            Soft Porn     2017         0.667   \n",
      "1                         Live from the Dentist Office     2015         0.529   \n",
      "2                                            The Spark     2017         0.479   \n",
      "3                                      Live In Concert     1993         0.512   \n",
      "4    Eight Miles High / Makes No Sense at All (Single)     1990         0.183   \n",
      "..                                                 ...      ...           ...   \n",
      "915               Sketches for My Sweetheart The Drunk     1998         0.458   \n",
      "916                                    Collective Soul     1995         0.733   \n",
      "917                                     Any Other City     2001         0.617   \n",
      "918                              Traficando Informação     1999         0.680   \n",
      "919                Gusttavo Lima E Você - Ao Vivo (CD)     2012         0.675   \n",
      "\n",
      "     energy   key  mode  valence    tempo  duration_ms  \n",
      "0     0.221  10.0   0.0    0.128   88.206       151920  \n",
      "1     0.713   2.0   1.0    0.516  167.368       195827  \n",
      "2     0.801   7.0   1.0    0.460  165.052       220447  \n",
      "3     0.405  10.0   0.0    0.516  159.745       198827  \n",
      "4     0.938   7.0   1.0    0.184  160.257       236093  \n",
      "..      ...   ...   ...      ...      ...          ...  \n",
      "915   0.721   5.0   1.0    0.342  180.743       279400  \n",
      "916   0.724   0.0   1.0    0.964  127.632       215867  \n",
      "917   0.718   7.0   1.0    0.280  134.790       353200  \n",
      "918   0.585   3.0   0.0    0.541   78.078       412395  \n",
      "919   0.958  11.0   0.0    0.813  128.006       201840  \n",
      "\n",
      "[920 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "#read metadata\n",
    "meta=pd.read_csv('features/Metadata/dev_metadata.csv')\n",
    "meta_test=pd.read_csv('features/Metadata/test_metadata.csv')\n",
    "print(meta)\n",
    "print(meta_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datapreparation read all audio data \n",
    "from functools import reduce\n",
    "\n",
    "audio_ivec=pd.read_csv('features/Audio/id_ivec512.csv')\n",
    "audio_mfcc_mean=pd.read_csv('features/Audio/id_mfcc_mean_cov.csv')\n",
    "audio_blf_chroma=pd.read_csv('features/Audio/id_chroma_bow.csv')\n",
    "audio_blf_bow=pd.read_csv('features/Audio/id_mfcc_bow.csv')\n",
    "audio_blf_emo_bow=pd.read_csv('features/Audio/id_emobase_bow.csv')\n",
    "\n",
    "\n",
    "audio_blf_correlation=pd.read_csv('features/Audio/id_blf_correlation.csv')\n",
    "audio_blf_logfluc=pd.read_csv('features/Audio/id_blf_logfluc.csv')\n",
    "audio_blf_deltaspectral=pd.read_csv('features/Audio/id_blf_deltaspectral.csv')\n",
    "audio_blf_spectralcontrast=pd.read_csv('features/Audio/id_blf_spectralcontrast.csv')\n",
    "audio_blf_vardeltaspectral=pd.read_csv('features/Audio/id_blf_vardeltaspectral.csv')\n",
    "audio_blf_spectral=pd.read_csv('features/Audio/id_blf_spectral.csv')\n",
    "\n",
    "\n",
    "#PCA for all high dimensional features \n",
    "\n",
    "number_of_comp=500\n",
    "\n",
    "columns = ['pca_comp_%i' % i\n",
    "   for i in range(number_of_comp)\n",
    "]\n",
    "\n",
    "#pca deltaspectral\n",
    "\n",
    "pca = PCA(n_components = number_of_comp).fit(audio_blf_deltaspectral.iloc[:,1:])\n",
    "pca_=pd.DataFrame(pca.transform(audio_blf_deltaspectral.iloc[:,1:]), columns = columns, index = audio_blf_deltaspectral.iloc[:,1:].index)\n",
    "\n",
    "\n",
    "audio_blf_deltaspectral_pca=pd.merge(audio_blf_deltaspectral.iloc[:,0], pca_, left_index=True, right_index=True)\n",
    "\n",
    "#pca spectralcontrast\n",
    "\n",
    "pca = PCA(n_components = number_of_comp).fit(audio_blf_spectralcontrast.iloc[:,1:])\n",
    "pca_=pd.DataFrame(pca.transform(audio_blf_spectralcontrast.iloc[:,1:]), columns = columns, index = audio_blf_spectralcontrast.iloc[:,1:].index)\n",
    "\n",
    "\n",
    "audio_blf_spectralcontrast_pca=pd.merge(audio_blf_spectralcontrast.iloc[:,0], pca_, left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#pca spectral\n",
    "\n",
    "pca = PCA(n_components = number_of_comp).fit(audio_blf_spectral.iloc[:,1:])\n",
    "pca_=pd.DataFrame(pca.transform(audio_blf_spectral.iloc[:,1:]), columns = columns, index = audio_blf_spectral.iloc[:,1:].index)\n",
    "\n",
    "\n",
    "audio_blf_spectral_pca=pd.merge(audio_blf_spectral.iloc[:,0], pca_, left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#pca vardeltaspectral\n",
    "\n",
    "pca = PCA(n_components = number_of_comp).fit(audio_blf_vardeltaspectral.iloc[:,1:])\n",
    "pca_=pd.DataFrame(pca.transform(audio_blf_vardeltaspectral.iloc[:,1:]), columns = columns, index = audio_blf_vardeltaspectral.iloc[:,1:].index)\n",
    "\n",
    "\n",
    "audio_blf_vardeltaspectral_pca=pd.merge(audio_blf_vardeltaspectral.iloc[:,0], pca_, left_index=True, right_index=True)\n",
    "\n",
    "#pca logfluc\n",
    "\n",
    "pca = PCA(n_components = number_of_comp).fit(audio_blf_logfluc.iloc[:,1:])\n",
    "pca_=pd.DataFrame(pca.transform(audio_blf_logfluc.iloc[:,1:]), columns = columns, index = audio_blf_logfluc.iloc[:,1:].index)\n",
    "\n",
    "\n",
    "audio_blf_logfluc_pca=pd.merge(audio_blf_logfluc.iloc[:,0], pca_, left_index=True, right_index=True)\n",
    "\n",
    "#PCA correlation rythm \n",
    "\n",
    "#################\n",
    "\n",
    "pca = PCA(n_components = number_of_comp).fit(audio_blf_correlation.iloc[:,1:])\n",
    "pca_=pd.DataFrame(pca.transform(audio_blf_correlation.iloc[:,1:]), columns = columns, index = audio_blf_correlation.iloc[:,1:].index)\n",
    "\n",
    "audio_blf_correlation_pca=pd.merge(audio_blf_correlation.iloc[:,0], pca_, left_index=True, right_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 ID           0             1             2             3  \\\n",
      "0  00GCd9HYEge6Ntwi  982.456954  44228.668526  66809.511633  98501.125775   \n",
      "1  01OcWE6ZDS0RAb4I  112.596728  12934.508389  30759.029167  50503.848806   \n",
      "2  01UK70YxnzhjXY29   97.340392  16132.629774  37557.072857  51455.529299   \n",
      "3  04elyRT3sHC2gYJK  121.809841  24879.904705  33416.528365  47053.649276   \n",
      "4  04k3rCH5GDaZmhj0  184.464204  33197.220437  44028.347025  53628.118065   \n",
      "\n",
      "               4              5              6              7              8  \\\n",
      "0  111127.250806  157984.695311  177905.792434  177317.806414  190182.745434   \n",
      "1   77877.105539   91510.659925  117381.737711  136558.839864  167493.429929   \n",
      "2   77917.108233   95041.055005  112850.760644  131092.701192  134267.005263   \n",
      "3   76098.495296   92065.165509  120847.935946  139434.747254  136824.224230   \n",
      "4   84538.617507   97556.916647  114746.585428  121355.210297  151081.940239   \n",
      "\n",
      "               9             10             11             12  \n",
      "0  208445.445767  202342.164836  197525.045711  159297.028577  \n",
      "1  153633.442159  149280.077027  150097.856235  130288.185456  \n",
      "2  135692.708519  135398.046634  129870.933934  105283.948458  \n",
      "3  139131.723992  167063.997519  150703.381033  145327.069353  \n",
      "4  143423.462077  150832.786279  143616.377157  119208.728149  \n"
     ]
    }
   ],
   "source": [
    "#prepare frame mfcc mean change rate\n",
    "\n",
    "audio_mfcc_frames=pd.read_csv('features/Audio/id_frame_mfcc.csv')\n",
    "audio_mfcc_frames_=audio_mfcc_frames\n",
    "audio_mfcc_frames_.iloc[:,2:]=audio_mfcc_frames_.iloc[:,2:].diff()**2\n",
    "hello=audio_mfcc_frames_.groupby(['ID'], as_index=False).sum()\n",
    "hello=hello.drop(columns='frame')\n",
    "print(hello.head())\n",
    "\n",
    "#need to implement BOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data considered for regression\n",
    "\n",
    "data_frames_1=[meta, audio_blf_bow, audio_blf_chroma, audio_ivec,\n",
    "audio_mfcc_mean,\n",
    "audio_blf_chroma,\n",
    "audio_blf_emo_bow, video_vgg_19]\n",
    "\n",
    "data_frames_1_test=[meta_test, audio_blf_bow, audio_blf_chroma, audio_ivec,\n",
    "audio_mfcc_mean,\n",
    "audio_blf_chroma,\n",
    "audio_blf_emo_bow]\n",
    "#data_frames_2=[audio_blf_correlation_pca, audio_blf_deltaspectral_pca, audio_blf_logfluc_pca] #audio_blf_spectralcontrast_pca, audio_blf_vardeltaspectral_pca, audio_blf_spectral_pca]\n",
    "\n",
    "\n",
    "data_frames_2=[audio_blf_logfluc, audio_blf_correlation, audio_blf_deltaspectral,\n",
    "               audio_blf_spectralcontrast, audio_blf_vardeltaspectral, audio_blf_spectral]\n",
    "\n",
    "data_frames=data_frames_1+data_frames_2\n",
    "\n",
    "data_frames_test=data_frames_1_test+data_frames_2\n",
    "\n",
    "df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['ID'],\n",
    "                                            how='inner'), data_frames)\n",
    "\n",
    "\n",
    "df_merged_test = reduce(lambda  left,right: pd.merge(left,right,on=['ID'],\n",
    "                                            how='inner'), data_frames_test)\n",
    "\n",
    "\n",
    "\n",
    "#ca = CCA()\n",
    "#ca.fit(audio_mfcc_mean.iloc[:, 1:], vgg_19.iloc[:, 1:])\n",
    "#X_c, Y_c = ca.transform(audio_mfcc_mean.iloc[:, 1:], vgg_19.iloc[:, 1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3678, 117)\n",
      "(3678, 19952)\n",
      "0.001 0.001 {'fit_time': array([2.0127275 , 1.77396798, 1.81513047, 1.97965097, 1.81899428]), 'score_time': array([0.85602593, 0.89795637, 0.89508677, 0.96634865, 0.91629028]), 'test_neg_mean_absolute_error': array([-11.23192399, -11.71306767, -11.45961275, -10.90541655,\n",
      "       -11.41127412]), 'test_neg_root_mean_squared_error': array([-14.12767006, -14.40593715, -14.29964815, -13.89822673,\n",
      "       -14.5831173 ])}\n",
      "0.001 0.1 {'fit_time': array([1.90983891, 1.82010674, 1.78284764, 1.8716352 , 2.07935882]), 'score_time': array([1.00395203, 0.88741994, 0.8872788 , 0.82267046, 1.06158328]), 'test_neg_mean_absolute_error': array([-11.23372232, -11.7149667 , -11.46144336, -10.90819968,\n",
      "       -11.4132494 ]), 'test_neg_root_mean_squared_error': array([-14.13008039, -14.40833688, -14.30180095, -13.90183552,\n",
      "       -14.58594697])}\n",
      "0.001 1 {'fit_time': array([4.16679811, 2.36106133, 2.19318938, 2.69990277, 2.44431567]), 'score_time': array([1.25188041, 1.13363481, 1.072124  , 1.06913185, 1.19133711]), 'test_neg_mean_absolute_error': array([-11.23372283, -11.71496739, -11.46144429, -10.90820068,\n",
      "       -11.41324966]), 'test_neg_root_mean_squared_error': array([-14.13008103, -14.40833766, -14.30180167, -13.9018365 ,\n",
      "       -14.58594751])}\n",
      "0.001 10 {'fit_time': array([3.22634649, 3.27003336, 3.32169867, 3.53877997, 3.60922575]), 'score_time': array([1.35146999, 1.48493719, 1.57573915, 1.58357477, 1.43970346]), 'test_neg_mean_absolute_error': array([-11.23372283, -11.71496739, -11.46144429, -10.90820068,\n",
      "       -11.41324966]), 'test_neg_root_mean_squared_error': array([-14.13008103, -14.40833766, -14.30180167, -13.9018365 ,\n",
      "       -14.58594751])}\n",
      "0.001 100 {'fit_time': array([3.51525712, 3.57807064, 3.54406643, 3.41171575, 2.82999492]), 'score_time': array([1.44561362, 1.47983432, 1.42751646, 1.42669272, 1.1541028 ]), 'test_neg_mean_absolute_error': array([-11.23372283, -11.71496739, -11.46144429, -10.90820068,\n",
      "       -11.41324966]), 'test_neg_root_mean_squared_error': array([-14.13008103, -14.40833766, -14.30180167, -13.9018365 ,\n",
      "       -14.58594751])}\n",
      "0.1 0.001 {'fit_time': array([1.73441792, 1.79437518, 1.83544564, 2.1187067 , 2.28141737]), 'score_time': array([0.82667446, 0.86879063, 1.05071902, 1.35056376, 1.01371837]), 'test_neg_mean_absolute_error': array([-11.0964078 , -11.5670288 , -11.31121684, -10.75845178,\n",
      "       -11.28718507]), 'test_neg_root_mean_squared_error': array([-13.92601904, -14.19936113, -14.12336301, -13.6719209 ,\n",
      "       -14.3962867 ])}\n",
      "0.1 0.1 {'fit_time': array([1.92731905, 2.11310482, 1.95331025, 2.00067425, 1.77847552]), 'score_time': array([0.92457628, 1.09175897, 1.17946839, 0.94184756, 0.83085966]), 'test_neg_mean_absolute_error': array([-11.23649132, -11.71682352, -11.46503557, -10.90874252,\n",
      "       -11.41704101]), 'test_neg_root_mean_squared_error': array([-14.12913457, -14.40658647, -14.2996468 , -13.90235078,\n",
      "       -14.59069836])}\n",
      "0.1 1 {'fit_time': array([1.89651775, 2.27565503, 1.8929646 , 1.80933142, 1.77807617]), 'score_time': array([0.91727257, 1.02140784, 0.85826373, 0.9068923 , 1.01132059]), 'test_neg_mean_absolute_error': array([-11.23654244, -11.71689468, -11.46513328, -10.90881803,\n",
      "       -11.41707735]), 'test_neg_root_mean_squared_error': array([-14.12919831, -14.40666253, -14.29971944, -13.90242662,\n",
      "       -14.59076677])}\n",
      "0.1 10 {'fit_time': array([2.66776133, 2.36265731, 2.69070363, 2.64387798, 2.67457223]), 'score_time': array([1.19738698, 1.16637087, 1.11821032, 1.13461208, 1.21181679]), 'test_neg_mean_absolute_error': array([-11.23654244, -11.71689468, -11.46513328, -10.90881803,\n",
      "       -11.41707735]), 'test_neg_root_mean_squared_error': array([-14.12919831, -14.40666253, -14.29971944, -13.90242662,\n",
      "       -14.59076677])}\n",
      "0.1 100 {'fit_time': array([2.60428572, 2.62446618, 2.78746104, 2.89769459, 3.02594495]), 'score_time': array([1.10399842, 1.32240152, 1.14612293, 1.33624387, 1.10708165]), 'test_neg_mean_absolute_error': array([-11.23654244, -11.71689468, -11.46513328, -10.90881803,\n",
      "       -11.41707735]), 'test_neg_root_mean_squared_error': array([-14.12919831, -14.40666253, -14.29971944, -13.90242662,\n",
      "       -14.59076677])}\n",
      "1 0.001 {'fit_time': array([1.77644515, 1.76274657, 1.79513955, 2.03748035, 1.68677449]), 'score_time': array([0.88155508, 0.89142394, 0.93151689, 0.86543274, 0.9554677 ]), 'test_neg_mean_absolute_error': array([-10.76525342, -11.19332788, -10.86393091, -10.37083175,\n",
      "       -10.94243115]), 'test_neg_root_mean_squared_error': array([-13.43525905, -13.70382284, -13.67015885, -13.08580616,\n",
      "       -13.84565091])}\n",
      "1 0.1 {'fit_time': array([1.97667909, 1.74344683, 1.77044415, 1.74756932, 1.82753539]), 'score_time': array([0.82264519, 0.8497107 , 0.98603582, 0.85184622, 0.89194179]), 'test_neg_mean_absolute_error': array([-11.23987033, -11.71823725, -11.46945873, -10.90358102,\n",
      "       -11.42065982]), 'test_neg_root_mean_squared_error': array([-14.12766638, -14.40427783, -14.29645342, -13.89750217,\n",
      "       -14.59513964])}\n",
      "1 1 {'fit_time': array([1.92875767, 1.92895246, 1.88973761, 1.81492043, 1.83018661]), 'score_time': array([0.87429547, 0.8325057 , 0.88389945, 0.85129642, 0.87425399]), 'test_neg_mean_absolute_error': array([-11.24040241, -11.71895556, -11.47043366, -10.90431954,\n",
      "       -11.42105442]), 'test_neg_root_mean_squared_error': array([-14.12829872, -14.40502198, -14.29715372, -13.89823811,\n",
      "       -14.5958693 ])}\n",
      "1 10 {'fit_time': array([2.83238602, 3.10608172, 3.38067889, 2.63951993, 3.16833305]), 'score_time': array([1.10227799, 1.41533041, 1.10315394, 1.06652069, 1.66170335]), 'test_neg_mean_absolute_error': array([-11.24040241, -11.71895556, -11.47043366, -10.90431954,\n",
      "       -11.42105442]), 'test_neg_root_mean_squared_error': array([-14.12829872, -14.40502198, -14.29715372, -13.89823811,\n",
      "       -14.5958693 ])}\n",
      "1 100 {'fit_time': array([3.12404656, 2.68529558, 2.67605925, 2.74155092, 2.75742626]), 'score_time': array([1.1446147 , 1.10324383, 1.10978889, 1.14982319, 1.18851638]), 'test_neg_mean_absolute_error': array([-11.24040241, -11.71895556, -11.47043366, -10.90431954,\n",
      "       -11.42105442]), 'test_neg_root_mean_squared_error': array([-14.12829872, -14.40502198, -14.29715372, -13.89823811,\n",
      "       -14.5958693 ])}\n",
      "10 0.001 {'fit_time': array([1.91579485, 1.85517716, 2.2704823 , 1.61184549, 1.15845418]), 'score_time': array([0.8605113 , 0.86136103, 0.88177776, 1.10590935, 0.4542706 ]), 'test_neg_mean_absolute_error': array([-10.56918194, -10.95915224, -10.55944001, -10.19191085,\n",
      "       -10.73912122]), 'test_neg_root_mean_squared_error': array([-13.14270446, -13.4668323 , -13.28053756, -12.77262175,\n",
      "       -13.49797122])}\n",
      "10 0.1 {'fit_time': array([  1.23354125, 420.9667201 ,   2.57105398,   3.15292287,\n",
      "         2.28881478]), 'score_time': array([0.58338881, 0.46386862, 2.32813644, 1.65711546, 1.04456806]), 'test_neg_mean_absolute_error': array([-11.24260779, -11.71832496, -11.46805827, -10.89415051,\n",
      "       -11.39913782]), 'test_neg_root_mean_squared_error': array([-14.12116136, -14.39450767, -14.28999647, -13.88172026,\n",
      "       -14.56431713])}\n",
      "10 1 {'fit_time': array([2.18939543, 2.17618108, 2.56613684, 1.57678437, 1.17486191]), 'score_time': array([0.87566066, 0.851722  , 0.85272145, 0.50165868, 0.44780254]), 'test_neg_mean_absolute_error': array([-11.24824305, -11.72432188, -11.47449575, -10.89952219,\n",
      "       -11.4015876 ]), 'test_neg_root_mean_squared_error': array([-14.12757054, -14.40148134, -14.29552761, -13.88778249,\n",
      "       -14.56985079])}\n",
      "10 10 {'fit_time': array([1.78821707, 1.74433756, 1.72339416, 1.73336816, 1.64759636]), 'score_time': array([0.59241486, 0.70710588, 0.63031292, 0.63230896, 0.69514155]), 'test_neg_mean_absolute_error': array([-11.24824305, -11.72432188, -11.47449577, -10.89952221,\n",
      "       -11.4015876 ]), 'test_neg_root_mean_squared_error': array([-14.12757054, -14.40148134, -14.29552762, -13.8877825 ,\n",
      "       -14.56985079])}\n",
      "10 100 {'fit_time': array([2.02358603, 1.79619884, 1.72139907, 1.88827896, 1.78960371]), 'score_time': array([0.602391  , 0.56349206, 0.7973702 , 0.68517923, 0.57513499]), 'test_neg_mean_absolute_error': array([-11.24824305, -11.72432188, -11.47449577, -10.89952221,\n",
      "       -11.4015876 ]), 'test_neg_root_mean_squared_error': array([-14.12757054, -14.40148134, -14.29552762, -13.8877825 ,\n",
      "       -14.56985079])}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.001 {'fit_time': array([1.32249165, 1.3686924 , 1.19959331, 1.15616941, 1.15879869]), 'score_time': array([0.77643847, 0.43621707, 0.47780561, 0.45525432, 0.45313096]), 'test_neg_mean_absolute_error': array([-10.63763096, -10.92709754, -10.56594592, -10.24380858,\n",
      "       -10.71885018]), 'test_neg_root_mean_squared_error': array([-13.29389641, -13.51107789, -13.28413006, -12.86011419,\n",
      "       -13.45203319])}\n",
      "100 0.1 {'fit_time': array([1.35096812, 1.52974725, 1.22730947, 1.42690754, 1.62649226]), 'score_time': array([0.56374097, 0.44626141, 0.41928864, 0.43475199, 0.62485528]), 'test_neg_mean_absolute_error': array([-11.27228883, -11.73996216, -11.5031171 , -10.88427409,\n",
      "       -11.36986584]), 'test_neg_root_mean_squared_error': array([-14.12889757, -14.3861861 , -14.28475653, -13.85656839,\n",
      "       -14.49690696])}\n",
      "100 1 {'fit_time': array([1.601753  , 1.57834816, 1.35197854, 1.52297401, 1.51299953]), 'score_time': array([0.51800346, 0.42231679, 0.43222785, 0.44287682, 0.45239091]), 'test_neg_mean_absolute_error': array([-11.28309989, -11.75087807, -11.51356698, -10.89238389,\n",
      "       -11.3755906 ]), 'test_neg_root_mean_squared_error': array([-14.14193173, -14.39952729, -14.29497585, -13.8665324 ,\n",
      "       -14.50751281])}\n",
      "100 10 {'fit_time': array([1.81131911, 1.73160601, 1.6945262 , 1.85554957, 2.03490472]), 'score_time': array([0.55657816, 0.58462167, 0.55948043, 0.55285048, 0.67909169]), 'test_neg_mean_absolute_error': array([-11.28309989, -11.75087807, -11.513567  , -10.89238391,\n",
      "       -11.3755906 ]), 'test_neg_root_mean_squared_error': array([-14.14193174, -14.39952729, -14.29497586, -13.8665324 ,\n",
      "       -14.50751281])}\n",
      "100 100 {'fit_time': array([2.71366811e+00, 3.28053403e+00, 3.48732114e+00, 2.04135036e+00,\n",
      "       3.55551714e+03]), 'score_time': array([1.03853393, 1.07244182, 1.26892543, 0.66925025, 3.72144866]), 'test_neg_mean_absolute_error': array([-11.28309989, -11.75087807, -11.513567  , -10.89238391,\n",
      "       -11.3755906 ]), 'test_neg_root_mean_squared_error': array([-14.14193174, -14.39952729, -14.29497586, -13.8665324 ,\n",
      "       -14.50751281])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([6.06827784, 1.84154606, 2.95398092, 0.69906616, 0.60319567]),\n",
       " 'score_time': array([0.00399661, 0.01198626, 0.0039947 , 0.00399399, 0.00399494]),\n",
       " 'test_neg_mean_absolute_error': array([-10.98729759, -11.52728301, -11.02354709, -11.42946869,\n",
       "        -10.96796896]),\n",
       " 'test_neg_root_mean_squared_error': array([-13.71745797, -14.50222408, -13.78671476, -14.30151305,\n",
       "        -13.69302116])}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Regression task, approach feature selection with linear regression and linear regression prediction\n",
    "\n",
    "#Train data\n",
    "X=df_merged.iloc[:,5:]\n",
    "\n",
    "#Normalize data \n",
    "scaler = StandardScaler()\n",
    "X= scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "pca_k=KernelPCA(n_components=500, kernel='rbf')\n",
    "projections=pca_k.fit_transform(X)\n",
    "\n",
    "#downproject data with PCA, not better\n",
    "#number_of_comp=50\n",
    "#columns = ['pca_comp_%i' % i\n",
    " #  for i in range(number_of_comp)]\n",
    "#pca = PCA(n_components = number_of_comp).fit(df_merged.iloc[:,6:])\n",
    "#X=pd.DataFrame(pca.transform(df_merged.iloc[:,6:]), columns = columns, index = df_merged.iloc[:,6:].index)\n",
    "\n",
    "#Labels\n",
    "\n",
    "y=df_merged['popularity']\n",
    "\n",
    "#feature selection does not work\n",
    "#univariate\n",
    "#X_train, X_test, y_train, y_test=train_test_split(X, y, random_state=0, test_size=0.5)\n",
    "#select=SelectPercentile(percentile=1)\n",
    "#select.fit(X, y)\n",
    "#X_selected=select.transform(X)\n",
    "\n",
    "#model-based feature selection\n",
    "select=SelectFromModel(LinearRegression(), threshold='4*mean')#5 mean\n",
    "select.fit(X, y)\n",
    "X_selected_2=select.transform(X)\n",
    "\n",
    "# print shape of train data\n",
    "print(X_selected_2.shape)\n",
    "print(X.shape)\n",
    "#Cross-validation\n",
    "#clf=LinearRegression()\n",
    "\n",
    "C_=[0.001, 0.1,1,10, 100]\n",
    "gamma_=[0.001, 0.1,1, 10, 100]\n",
    "for C in C_:\n",
    "    for gamma in gamma_:\n",
    "        clf=SVR(C=C, gamma=gamma)\n",
    "        #clf=RandomForestRegressor(n_estimators=1001)\n",
    "        scores=cross_validate(clf, X_selected_2, y, scoring=['neg_mean_absolute_error',\n",
    "        'neg_root_mean_squared_error'])\n",
    "        print(C, gamma, scores)\n",
    "\n",
    "clf=LinearRegression()\n",
    "\n",
    "scores=cross_validate(clf, projections, y, scoring=['neg_mean_absolute_error',\n",
    "        'neg_root_mean_squared_error'])\n",
    "scores\n",
    "\n",
    "\n",
    "#clf.fit(X_selected_2, y)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.04188848, 0.05086565, 0.0398922 , 0.036901  , 0.04188657]), 'score_time': array([0.00897622, 0.00199366, 0.00199461, 0.00199556, 0.00199556]), 'test_neg_mean_absolute_error': array([-10.91211551, -11.1162842 , -11.11951655, -10.54948933,\n",
      "       -10.84785627]), 'test_neg_root_mean_squared_error': array([-13.75800114, -13.77126853, -13.91362917, -13.26259626,\n",
      "       -13.74425904])}\n",
      "Mean absolute error 11.358564111935198:\n",
      "Mean squared error 14.239037505874942:\n"
     ]
    }
   ],
   "source": [
    "#Baseline for regression\n",
    "\n",
    "print(scores)\n",
    "\n",
    "baseline=np.full(y.shape, np.mean(y))\n",
    "print('Mean absolute error {0}:'.format(mean_absolute_error(y, baseline)))\n",
    "print('Mean squared error {0}:'.format(mean_squared_error(y, baseline, squared=False)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##predictions\n",
    "\n",
    "X_pred=df_merged_test.iloc[:,4:]\n",
    "scaler = StandardScaler()\n",
    "X_pred= scaler.fit_transform(X_pred)\n",
    "\n",
    "X_pred=select.transform(X_pred)\n",
    "\n",
    "print(X.shape)\n",
    "print(X_pred.shape)\n",
    "#Normalize data \n",
    "\n",
    "\n",
    "prediction=pd.DataFrame(clf.predict(X_pred))\n",
    "predictions_df=pd.concat([meta_test['ID'], prediction], axis=1)\n",
    "\n",
    "print(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data frames for tagging. Handgrafting shows that this combination gives the best result (Parameter tune?)\n",
    "\n",
    "\n",
    "data_frames_1=[meta, audio_blf_chroma, audio_blf_bow, audio_ivec, audio_blf_emo_bow, video_vgg_19]\n",
    "#data_frames_2=[audio_blf_logfluc_pca, audio_blf_correlation_pca,\n",
    " #               audio_blf_spectralcontrast_pca]\n",
    "\n",
    "data_frames_2=[audio_blf_logfluc, audio_blf_correlation, audio_blf_deltaspectral,\n",
    "              audio_blf_spectralcontrast, audio_blf_vardeltaspectral, audio_blf_spectral]\n",
    "\n",
    "data_frames=data_frames_1+data_frames_2\n",
    "#audio_blf_bow good, logfluc does something, audio_blf (PCA 500; ca.10)\n",
    "#\n",
    "#print(vgg_19)\n",
    "\n",
    "#\n",
    "#data_frames=[meta, audio_mfcc_mean, audio_blf_bow] \n",
    "            # video_vgg_19]\n",
    "\n",
    "data_frames_test=[meta_test, audio_mfcc_mean, audio_blf_bow]\n",
    "\n",
    "df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['ID'],\n",
    "                                            how='inner'), data_frames)\n",
    "\n",
    "\n",
    "df_merged_test = reduce(lambda  left,right: pd.merge(left,right,on=['ID'],\n",
    "                                            how='inner'), data_frames_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\p41999\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\multiclass.py:80: UserWarning: Label not 183 is present in all training examples.\n",
      "  warnings.warn(\"Label %s is present in all training examples.\" %\n",
      "c:\\users\\p41999\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\multiclass.py:80: UserWarning: Label not 72 is present in all training examples.\n",
      "  warnings.warn(\"Label %s is present in all training examples.\" %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([7.60992646, 6.73838615, 6.22823429, 6.36478734, 6.34550261]),\n",
       " 'score_time': array([0.23517537, 0.16807818, 0.15234303, 0.1499486 , 0.15691924]),\n",
       " 'test_f1_micro': array([0.02521566, 0.02713178, 0.02967742, 0.02238315, 0.03006536]),\n",
       " 'test_precision_micro': array([0.67857143, 0.72413793, 0.6969697 , 0.68      , 0.79310345]),\n",
       " 'test_recall_micro': array([0.01284652, 0.01382488, 0.0151615 , 0.01137885, 0.01532312])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tagging\n",
    "\n",
    "#preparing tag data, PCA down-projection did not work \n",
    "\n",
    "#number_of_comp=500\n",
    "#columns = ['pca_comp_%i' % i\n",
    "#for i in range(number_of_comp)]\n",
    "#pca = PCA(n_components = number_of_comp).fit(df_merged.iloc[:,6:])\n",
    "#X=pd.DataFrame(pca.transform(df_merged.iloc[:,6:]), columns = columns, index = df_merged.iloc[:,6:].index)\n",
    "\n",
    "#define and prepare tags for multilabel classification \n",
    "\n",
    "tags_df=pd.read_csv('features/Metadata/dev_tags.csv')\n",
    "\n",
    "tags_df['tags_new']=tags_df['tags']\n",
    "\n",
    "\n",
    "for ii in range(len(tags_df['tags'])):\n",
    "    tagi=tags_df['tags'][ii]\n",
    "    tagi=tagi.replace('[','')\n",
    "    tagi=tagi.replace(']','')\n",
    "    tagi=tagi.replace(' ','')\n",
    "    tagi=tagi.split(\",\")\n",
    "    tags_df['tags_new'][ii]=tagi\n",
    "    \n",
    "\n",
    "df_merged_tags=df_merged.merge(tags_df, on=['ID'])\n",
    "\n",
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "multilabel_binarizer.fit(df_merged_tags['tags_new'])\n",
    "\n",
    "\n",
    "# normalize features and transform target variable\n",
    "\n",
    "X=df_merged_tags.iloc[:,5:-2]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X= scaler.transform(X)\n",
    "\n",
    "pca_k=KernelPCA(n_components=100, kernel='rbf')\n",
    "projections=pca_k.fit_transform(X)\n",
    "\n",
    "y = multilabel_binarizer.transform(df_merged_tags['tags_new'])\n",
    "\n",
    "#feature selection did not work \n",
    "#from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "#selected_features = [] \n",
    "#for label in range(y.shape[1]):\n",
    " #   selector = SelectKBest(chi2, k='all')\n",
    "  #  selector.fit(X, y[:,label])\n",
    "   # selected_features.append(list(selector.scores_))\n",
    "\n",
    "#print(selected_features)\n",
    "\n",
    "#selected_features = np.mean(selected_features, axis=0) > 0.3\n",
    "#selected_features = np.max(selected_features, axis=0) > threshold\n",
    "\n",
    "#make predictions\n",
    "\n",
    "#lr=svm.SVC()\n",
    "#clf = MultiOutputClassifier(svm.SVC()).fit(X, y)\n",
    "lr = LogisticRegression(max_iter=100)\n",
    "#lr = neighbors.KNeighborsClassifier(15)\n",
    "clf_lab = OneVsRestClassifier(lr)\n",
    "\n",
    "scores=cross_validate(clf_lab, projections, y, scoring=['f1_micro', 'precision_micro',\n",
    "'recall_micro'])\n",
    "\n",
    "scores\n",
    "\n",
    "#clf_lab.fit(X,y)\n",
    "\n",
    "#clf = neighbors.KNeighborsClassifier(15)\n",
    "#clf = MLkNN(k=15)\n",
    "\n",
    "\n",
    "#Define baseline \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3678, 100)\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "This OneVsRestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-72041cb69a1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mX_pred\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mprediction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclf_lab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;31m#prediction=multilabel_binarizer.inverse_transform(prediction)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\p41999\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\multiclass.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    356\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0mmulti\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;32mclass\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m         \"\"\"\n\u001b[1;32m--> 358\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\p41999\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\p41999\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This OneVsRestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "#predicting labels \n",
    "print(projections.shape)\n",
    "\n",
    "X_pred=df_merged_test.iloc[:,4:]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_pred)\n",
    "X_pred= scaler.transform(X_pred)\n",
    "\n",
    "prediction=clf_lab.predict(X_pred)\n",
    "#prediction=multilabel_binarizer.inverse_transform(prediction)\n",
    "\n",
    "predictions_df=meta_test['ID']\n",
    "\n",
    "prediction=pd.DataFrame(prediction)\n",
    "predictions_df=pd.concat([meta_test['ID'], prediction], axis=1)\n",
    "print(predictions_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.07721186334626454\n",
      "0.05 0.21631530705774518\n",
      "0.1 0.25998874507597075\n",
      "0.15 0.2534351145038167\n",
      "0.17 0.25102543068088595\n",
      "0.2 0.22121486854034453\n",
      "0.25 0.17961654894046417\n",
      "0.3 0.1471861471861472\n",
      "0.4 0.06954436450839328\n",
      "0.5 0.027568922305764413\n",
      "0.010209352909189777\n"
     ]
    }
   ],
   "source": [
    "#Tunning decision threshold\n",
    "\n",
    "for t in [0.01, 0.05, 0.1, 0.15, 0.17, 0.20, 0.25, 0.30, 0.40, 0.5]: \n",
    "    # threshold value\n",
    "\n",
    "    X_train, X_test, y_train, y_test=train_test_split(projections, y, random_state=0, test_size=0.1)\n",
    "    clf_lab.fit(X_train, y_train)\n",
    "    y_pred_prob = clf_lab.predict_proba(X_test)\n",
    "    y_pred_new = (y_pred_prob >= t).astype(int)\n",
    "    print(t, f1_score(y_test, y_pred_new, average=\"micro\"))\n",
    "    #scores\n",
    "\n",
    "\n",
    "print(y.mean(axis=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multilabel_binarizer.inverse_transform(y_pred)\n",
    "\n",
    "#Baseline label\n",
    "\n",
    "print(y.mean(axis=0))\n",
    "print(y.sum(axis=1))\n",
    "print(y.shape)\n",
    "#for ii in \n",
    "dummy=np.zeros(y.shape)\n",
    "for ii in range(y.shape[1]):\n",
    "    proba=y.mean(axis=0)[ii]\n",
    "    dummy[:,ii]=np.random.binomial(1, proba, y.shape[0])\n",
    "\n",
    "print('Baseline f1: {0}'.format(f1_score(y, dummy, average='micro')))\n",
    "print('Baseline precision: {0}'.format(precision_score(y, dummy, average='micro')))\n",
    "print('Baseline recall: {0}'.format(recall_score(y, dummy, average='micro')))\n",
    "\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "clf_d = OneVsRestClassifier(dummy_clf)\n",
    "clf_d.fit(X, y)\n",
    "clf_d.predict(X)\n",
    "\n",
    "print(clf_d.score(X, y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#issue predictions on the test set\n",
    "\n",
    "#Merge with test IDs\n",
    "#Adjust threshold in crossvalidation\n",
    "#BOF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from random import randrange\n",
    "from random import seed\n",
    " \n",
    "# calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "    distance = 0.0\n",
    "    for i in range(len(row1)-1):\n",
    "        distance += (row1[i] - row2[i])**2\n",
    "    return sqrt(distance)\n",
    " \n",
    "# Locate the best matching unit\n",
    "def get_best_matching_unit(codebooks, test_row):\n",
    "    distances = list()\n",
    "    for codebook in codebooks:\n",
    "        dist = euclidean_distance(codebook, test_row)\n",
    "        distances.append((codebook, dist))\n",
    "    distances.sort(key=lambda tup: tup[1])\n",
    "    return distances[0][0]\n",
    " \n",
    "# Create a random codebook vector\n",
    "def random_codebook(train):\n",
    "    n_records = len(train)\n",
    "    n_features = len(train[0])\n",
    "    codebook = [train[randrange(n_records)][i] for i in range(n_features)]\n",
    "    return codebook\n",
    " \n",
    "# Train a set of codebook vectors\n",
    "def train_codebooks(train, n_codebooks, lrate, epochs):\n",
    "    codebooks = [random_codebook(train) for i in range(n_codebooks)]\n",
    "    for epoch in range(epochs):\n",
    "        rate = lrate * (1.0-(epoch/float(epochs)))\n",
    "        sum_error = 0.0\n",
    "        for row in train:\n",
    "            bmu = get_best_matching_unit(codebooks, row)\n",
    "            for i in range(len(row)-1):\n",
    "                error = row[i] - bmu[i]\n",
    "                sum_error += error**2\n",
    "                if bmu[-1] == row[-1]:\n",
    "                    bmu[i] += rate * error\n",
    "                else:\n",
    "                    bmu[i] -= rate * error\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, rate, sum_error))\n",
    "    return codebooks\n",
    " \n",
    "# Test the training function\n",
    "seed(1)\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "           [1.465489372,2.362125076,0],\n",
    "    [3.396561688,4.400293529,0],\n",
    "    [1.38807019,1.850220317,0],\n",
    "    [3.06407232,3.005305973,0],\n",
    "    [7.627531214,2.759262235,1],\n",
    "    [5.332441248,2.088626775,1],\n",
    "    [6.922596716,1.77106367,1],\n",
    "    [8.675418651,-0.242068655,1],\n",
    "    [7.673756466,3.508563011,1]]\n",
    "learn_rate = 0.3\n",
    "n_epochs = 10\n",
    "n_codebooks = 500\n",
    "\n",
    "\n",
    "\n",
    "codebooks = train_codebooks(audio_mfcc_frames.iloc[:, 1:].values.tolist(), n_codebooks, learn_rate, n_epochs)\n",
    "print('Codebooks: %s' % codebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blf correlation\n",
    "audio_blf_correlation=pd.read_csv('features/Audio/id_blf_correlation.csv')\n",
    "\n",
    "print(audio_blf_correlation)\n",
    "audio_blf_correlation['mean_cor']=audio_blf_correlation.mean(axis=1)\n",
    "audio_blf_correlation['std_cor']=audio_blf_correlation.std(axis=1)\n",
    "\n",
    "meta_audio=meta.merge(audio_blf_correlation, on='ID')\n",
    "\n",
    "print(meta_audio)\n",
    "X=meta_audio.iloc[:,14:]\n",
    "\n",
    "y=meta_audio['popularity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.844682884200431"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Video \n",
    "\n",
    "video_vgg_19=pd.read_csv('features/id_vgg19_agg.csv')\n",
    "meta_video=video_vgg_19.merge(meta, on='ID')\n",
    "meta_video\n",
    "X=meta_video.iloc[:,1:3000]\n",
    "y=meta_video['popularity']\n",
    "\n",
    "reg = LinearRegression().fit(X, y)\n",
    "reg.score(X, y)\n",
    "\n",
    "#low effect size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare textdata\n",
    "data_dir='features/Text/Lyrics'\n",
    "\n",
    "paths=os.listdir(data_dir)\n",
    "data = []\n",
    "\n",
    "#for ii in range(3): \n",
    "for ii in range(len(paths)): \n",
    "    full_path = os.path.join(data_dir, paths[ii])\n",
    "    with open(full_path, 'r', encoding=\"utf8\") as file:\n",
    "        dat = file.read().replace('\\n', '')\n",
    "    data.append([paths[ii].split('.')[0], dat])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['ID', 'Lyric'])\n",
    "\n",
    "df.merge(meta, on='ID')\n",
    "#Merging of dataframes \n",
    "\n",
    "\n",
    "vgg_19=pd.read_csv('features/id_vgg19_agg.csv')\n",
    "\n",
    "number_of_comp=500\n",
    "\n",
    "columns = ['pca_comp_%i' % i\n",
    "   for i in range(number_of_comp)\n",
    "]\n",
    "\n",
    "#pca vgg_19\n",
    "\n",
    "pca = PCA(n_components = number_of_comp).fit(vgg_19.iloc[:,1:])\n",
    "pca_=pd.DataFrame(pca.transform(vgg_19.iloc[:,1:]), columns = columns, index = vgg_19.iloc[:,1:].index)\n",
    "vgg_19_pca=pd.merge(vgg_19.iloc[:,0], pca_, left_index=True, right_index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
